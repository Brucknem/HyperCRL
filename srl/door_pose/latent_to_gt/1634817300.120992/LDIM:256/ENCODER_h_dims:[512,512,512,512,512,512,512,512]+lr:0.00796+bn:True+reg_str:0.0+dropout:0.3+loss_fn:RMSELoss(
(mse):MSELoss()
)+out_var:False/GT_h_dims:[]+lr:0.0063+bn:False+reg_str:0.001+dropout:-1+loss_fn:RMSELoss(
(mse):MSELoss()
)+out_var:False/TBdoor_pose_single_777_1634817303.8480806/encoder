digraph {
	graph [size="33.449999999999996,33.449999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140578182632864 [label="
 (10, 256)" fillcolor=darkolivegreen1]
	140573391144272 [label=LeakyReluBackward0]
	140578182630704 -> 140573391144272
	140578182630704 [label=AddmmBackward]
	140578182630648 -> 140578182630704
	140578182514296 [label="linear.8.bias
 (256)" fillcolor=lightblue]
	140578182514296 -> 140578182630648
	140578182630648 [label=AccumulateGrad]
	140578182630480 -> 140578182630704
	140578182630480 [label=FusedDropoutBackward]
	140578182630368 -> 140578182630480
	140578182630368 [label=NativeBatchNormBackward]
	140578182630256 -> 140578182630368
	140578182630256 [label=LeakyReluBackward0]
	140578182630032 -> 140578182630256
	140578182630032 [label=AddmmBackward]
	140578182629920 -> 140578182630032
	140578182514152 [label="linear.7.bias
 (512)" fillcolor=lightblue]
	140578182514152 -> 140578182629920
	140578182629920 [label=AccumulateGrad]
	140578182629864 -> 140578182630032
	140578182629864 [label=FusedDropoutBackward]
	140578182629696 -> 140578182629864
	140578182629696 [label=NativeBatchNormBackward]
	140578182629584 -> 140578182629696
	140578182629584 [label=LeakyReluBackward0]
	140578182629360 -> 140578182629584
	140578182629360 [label=AddmmBackward]
	140578182629248 -> 140578182629360
	140578182514008 [label="linear.6.bias
 (512)" fillcolor=lightblue]
	140578182514008 -> 140578182629248
	140578182629248 [label=AccumulateGrad]
	140578182629192 -> 140578182629360
	140578182629192 [label=FusedDropoutBackward]
	140578182629024 -> 140578182629192
	140578182629024 [label=NativeBatchNormBackward]
	140578182628912 -> 140578182629024
	140578182628912 [label=LeakyReluBackward0]
	140578182628688 -> 140578182628912
	140578182628688 [label=AddmmBackward]
	140578182628576 -> 140578182628688
	140578182513864 [label="linear.5.bias
 (512)" fillcolor=lightblue]
	140578182513864 -> 140578182628576
	140578182628576 [label=AccumulateGrad]
	140578182628520 -> 140578182628688
	140578182628520 [label=FusedDropoutBackward]
	140578182628352 -> 140578182628520
	140578182628352 [label=NativeBatchNormBackward]
	140578182628240 -> 140578182628352
	140578182628240 [label=LeakyReluBackward0]
	140578182628016 -> 140578182628240
	140578182628016 [label=AddmmBackward]
	140578182627904 -> 140578182628016
	140578182513720 [label="linear.4.bias
 (512)" fillcolor=lightblue]
	140578182513720 -> 140578182627904
	140578182627904 [label=AccumulateGrad]
	140578182627848 -> 140578182628016
	140578182627848 [label=FusedDropoutBackward]
	140578182630816 -> 140578182627848
	140578182630816 [label=NativeBatchNormBackward]
	140578182630928 -> 140578182630816
	140578182630928 [label=LeakyReluBackward0]
	140578182631152 -> 140578182630928
	140578182631152 [label=AddmmBackward]
	140578182631264 -> 140578182631152
	140578182513576 [label="linear.3.bias
 (512)" fillcolor=lightblue]
	140578182513576 -> 140578182631264
	140578182631264 [label=AccumulateGrad]
	140578182631320 -> 140578182631152
	140578182631320 [label=FusedDropoutBackward]
	140578177843328 -> 140578182631320
	140578177843328 [label=NativeBatchNormBackward]
	140578177843440 -> 140578177843328
	140578177843440 [label=LeakyReluBackward0]
	140578177843664 -> 140578177843440
	140578177843664 [label=AddmmBackward]
	140578177843776 -> 140578177843664
	140578182513432 [label="linear.2.bias
 (512)" fillcolor=lightblue]
	140578182513432 -> 140578177843776
	140578177843776 [label=AccumulateGrad]
	140578177843832 -> 140578177843664
	140578177843832 [label=FusedDropoutBackward]
	140578177844000 -> 140578177843832
	140578177844000 [label=NativeBatchNormBackward]
	140578177844112 -> 140578177844000
	140578177844112 [label=LeakyReluBackward0]
	140578177844336 -> 140578177844112
	140578177844336 [label=AddmmBackward]
	140578177844448 -> 140578177844336
	140578182513288 [label="linear.1.bias
 (512)" fillcolor=lightblue]
	140578182513288 -> 140578177844448
	140578177844448 [label=AccumulateGrad]
	140578177844504 -> 140578177844336
	140578177844504 [label=FusedDropoutBackward]
	140578177844672 -> 140578177844504
	140578177844672 [label=NativeBatchNormBackward]
	140578177844784 -> 140578177844672
	140578177844784 [label=LeakyReluBackward0]
	140578177845008 -> 140578177844784
	140578177845008 [label=AddmmBackward]
	140578177845176 -> 140578177845008
	140578182513144 [label="linear.0.bias
 (512)" fillcolor=lightblue]
	140578182513144 -> 140578177845176
	140578177845176 [label=AccumulateGrad]
	140578177845232 -> 140578177845008
	140578177845232 [label=TBackward]
	140578177845400 -> 140578177845232
	140578182513072 [label="linear.0.weight
 (512, 512)" fillcolor=lightblue]
	140578182513072 -> 140578177845400
	140578177845400 [label=AccumulateGrad]
	140578177844840 -> 140578177844672
	140578182457456 [label="bn.0.weight
 (512)" fillcolor=lightblue]
	140578182457456 -> 140578177844840
	140578177844840 [label=AccumulateGrad]
	140578177844896 -> 140578177844672
	140578182457528 [label="bn.0.bias
 (512)" fillcolor=lightblue]
	140578182457528 -> 140578177844896
	140578177844896 [label=AccumulateGrad]
	140578177844560 -> 140578177844336
	140578177844560 [label=TBackward]
	140578177844728 -> 140578177844560
	140578182513216 [label="linear.1.weight
 (512, 512)" fillcolor=lightblue]
	140578182513216 -> 140578177844728
	140578177844728 [label=AccumulateGrad]
	140578177844168 -> 140578177844000
	140573391177408 [label="bn.1.weight
 (512)" fillcolor=lightblue]
	140573391177408 -> 140578177844168
	140578177844168 [label=AccumulateGrad]
	140578177844224 -> 140578177844000
	140573391108208 [label="bn.1.bias
 (512)" fillcolor=lightblue]
	140573391108208 -> 140578177844224
	140578177844224 [label=AccumulateGrad]
	140578177843888 -> 140578177843664
	140578177843888 [label=TBackward]
	140578177844056 -> 140578177843888
	140578182513360 [label="linear.2.weight
 (512, 512)" fillcolor=lightblue]
	140578182513360 -> 140578177844056
	140578177844056 [label=AccumulateGrad]
	140578177843496 -> 140578177843328
	140578319951408 [label="bn.2.weight
 (512)" fillcolor=lightblue]
	140578319951408 -> 140578177843496
	140578177843496 [label=AccumulateGrad]
	140578177843552 -> 140578177843328
	140578319953424 [label="bn.2.bias
 (512)" fillcolor=lightblue]
	140578319953424 -> 140578177843552
	140578177843552 [label=AccumulateGrad]
	140578182631376 -> 140578182631152
	140578182631376 [label=TBackward]
	140578177843384 -> 140578182631376
	140578182513504 [label="linear.3.weight
 (512, 512)" fillcolor=lightblue]
	140578182513504 -> 140578177843384
	140578177843384 [label=AccumulateGrad]
	140578182630984 -> 140578182630816
	140578182457960 [label="bn.3.weight
 (512)" fillcolor=lightblue]
	140578182457960 -> 140578182630984
	140578182630984 [label=AccumulateGrad]
	140578182631040 -> 140578182630816
	140578182458032 [label="bn.3.bias
 (512)" fillcolor=lightblue]
	140578182458032 -> 140578182631040
	140578182631040 [label=AccumulateGrad]
	140578182627792 -> 140578182628016
	140578182627792 [label=TBackward]
	140578182630872 -> 140578182627792
	140578182513648 [label="linear.4.weight
 (512, 512)" fillcolor=lightblue]
	140578182513648 -> 140578182630872
	140578182630872 [label=AccumulateGrad]
	140578182628184 -> 140578182628352
	140578182458320 [label="bn.4.weight
 (512)" fillcolor=lightblue]
	140578182458320 -> 140578182628184
	140578182628184 [label=AccumulateGrad]
	140578182628128 -> 140578182628352
	140578182458392 [label="bn.4.bias
 (512)" fillcolor=lightblue]
	140578182458392 -> 140578182628128
	140578182628128 [label=AccumulateGrad]
	140578182628464 -> 140578182628688
	140578182628464 [label=TBackward]
	140578182628296 -> 140578182628464
	140578182513792 [label="linear.5.weight
 (512, 512)" fillcolor=lightblue]
	140578182513792 -> 140578182628296
	140578182628296 [label=AccumulateGrad]
	140578182628856 -> 140578182629024
	140578182458680 [label="bn.5.weight
 (512)" fillcolor=lightblue]
	140578182458680 -> 140578182628856
	140578182628856 [label=AccumulateGrad]
	140578182628800 -> 140578182629024
	140578182458752 [label="bn.5.bias
 (512)" fillcolor=lightblue]
	140578182458752 -> 140578182628800
	140578182628800 [label=AccumulateGrad]
	140578182629136 -> 140578182629360
	140578182629136 [label=TBackward]
	140578182628968 -> 140578182629136
	140578182513936 [label="linear.6.weight
 (512, 512)" fillcolor=lightblue]
	140578182513936 -> 140578182628968
	140578182628968 [label=AccumulateGrad]
	140578182629528 -> 140578182629696
	140578182459040 [label="bn.6.weight
 (512)" fillcolor=lightblue]
	140578182459040 -> 140578182629528
	140578182629528 [label=AccumulateGrad]
	140578182629472 -> 140578182629696
	140578182459112 [label="bn.6.bias
 (512)" fillcolor=lightblue]
	140578182459112 -> 140578182629472
	140578182629472 [label=AccumulateGrad]
	140578182629808 -> 140578182630032
	140578182629808 [label=TBackward]
	140578182629640 -> 140578182629808
	140578182514080 [label="linear.7.weight
 (512, 512)" fillcolor=lightblue]
	140578182514080 -> 140578182629640
	140578182629640 [label=AccumulateGrad]
	140578182630200 -> 140578182630368
	140578182512712 [label="bn.7.weight
 (512)" fillcolor=lightblue]
	140578182512712 -> 140578182630200
	140578182630200 [label=AccumulateGrad]
	140578182630144 -> 140578182630368
	140578182512784 [label="bn.7.bias
 (512)" fillcolor=lightblue]
	140578182512784 -> 140578182630144
	140578182630144 [label=AccumulateGrad]
	140578182630536 -> 140578182630704
	140578182630536 [label=TBackward]
	140578182630312 -> 140578182630536
	140578182514224 [label="linear.8.weight
 (256, 512)" fillcolor=lightblue]
	140578182514224 -> 140578182630312
	140578182630312 [label=AccumulateGrad]
	140573391144272 -> 140578182632864
}
